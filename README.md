# Лабораторная № 2

## Раздел
Работа с памятью в Java

## Описание
В этой работе студентам предстоит реализовать одну из изученных ранее структур данных поиска с применением внешней памяти для Java, а также провести анализ производительности и оптимизацию, если потребуется.

## Структура данных
Для реализации была выбрана структура данных ball tree

## Начальная реализация (tag v1)

### Benchmark

![v1_benchmark.png](assets%2Fv1_benchmark.png)

Максимальное значение n(количество векторов) взяла за 1000000 - само число, по факту, в инт влезает, но так как вектора многоразмерные, то файл с 1000000 векторами занимает 2 ГБ и в память не влезает, что будем считать достаточным основанием для использования алгоритма с внешней памятью.

На бенчмарке достаточно ожидаемо видим что при увеличении размера ноды(в случае когда он меньше чем n) скорость немного растет(по меньшему количеству файликов надо ходить), поэтому для экспериментов с поиском будем брать наибольший n и наибольший leafSize.

### CPU flamegraph

![v1_cpu_flamegraph.png](assets%2Fv1_cpu_flamegraph.png)

### Mem usage flamegraph

![v1_mem_flamegraph.png](assets%2Fv1_mem_flamegraph.png)

На чтение векторов из файлов уходит около 57% процентов времени по ЦПУ и почти 100% по памяти. На открытие файлов уходит только 3%, что в целом неплохо, так что нет смысла делать размер ноды еще больше - но можно попробовать считать вектора побыстрее - не по одному, а пакетом.

## Оптимизированная реализация (tag v2)

### Benchmark

![v2_benchmark.png](assets%2Fv2_benchmark.png)

Для всех параметров скорость увеличилась - в частности, для наибольшего n throughput увеличился с 0.537 до 0.856 ops/s. Так же видим что при росте размера ноды скорость более выраженно растет по сравнению с прошлой реализацией, так как теперь считывание векторов с конечной ноды происходит намного быстрее. 

### CPU flamegraph

![v2_parallel_cpu_flamegraph.png](assets%2Fv2_parallel_cpu_flamegraph.png)

### Mem usage flamegraph

![v2_mem_flamegraph.png](assets%2Fv2_mem_flamegraph.png)

По памяти теперь чтение векторов занимает только 50% вместое 100% за счет переиспользования буффера, по ЦПУ занимает 25% вместо 50%. 

### Пара экспериментов по дороге
Как итоговая реализация победило параллельное вычитывание векторов из массива чисел, считанного из файлов и затем последовательный инсерт в очередь. Однако была еще пара реализаций:
1. Последовательное вычитывание векторов. Это позволило бы не создавать промежуточный лист, а передавать сразу стрим в результате функции `readVectorsFromFile()`. Такая реализация оказалась немного медленнее - 0.75 ops/s против 0.85 ops/s: затраты на сборку в стрима в лист все-таки были покрыты.
2. MinMaxPriorityQueue.addAll(). Эта очередь из коробки умеет оставлять наименьшие элементы, даже если передать ей больше элементов черем определенный размер очереди - нельзя ведь так взять и не попробовать коробочное решение. Но как оказалось оно намного медленнее работает - под капотом операция добавления в очередь очень дорогая, руками проверить нужен ли нам этот элемент намного быстрее. Результат такой реализации - 0.13 ops/s.

## Выводы

Сам вывод простой - в алгоритме с внешней памятью надо хорошо устроить работу с диском, из чего следует что стоит читать(да и писать) батчами. Ну и еще надо следить за аллокациями, переиспользовать буфферы там где это возможно - память мало того что память занимает, так еще и время тоже нужно на ее выделение(все это достаточно очевидно, но хз на самом деле что еще тут можно придумать).

Небольшая заметка-вопрос по реализации
1. Насколько я знаю RAF является предназначен как раз для того чтоб читать\писать не последовательно, что и происходит при построении дерева. При поиске однако мы последовательно читаем, поэтому, возможно, другая реализация была бы эффективнее - если есть мысли что могло бы быть лучше, буду рада услышать)
